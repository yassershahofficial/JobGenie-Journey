# ETL Pipeline - O*NET Data Processing

A two-stage ETL (Extract, Transform, Load) pipeline that processes O*NET database Excel files into structured JSON job profiles for use in the job matching system.

## Overview

The ETL pipeline consists of two main stages:

1. **Data Cleaning** (`data_cleaning.py`) - Filters and cleans raw O*NET Excel data
2. **Transformation** (`transformation.py`) - Transforms cleaned data into structured JSON job profiles

The pipeline processes O*NET database files to extract:
- Job titles and descriptions
- RIASEC personality vectors (6-dimensional)
- Knowledge domains (for degree/major matching)
- Technology skills (for resume/CV matching)
- Job zones (education/experience requirements)

## Prerequisites

- Python 3.10+
- O*NET database Excel files (generated by `onet_job_scraping` module)
- Required packages: `pandas`, `openpyxl`

## Installation

The ETL pipeline uses the shared project virtual environment:

```bash
# Activate virtual environment
.\venv\Scripts\Activate.ps1

# Dependencies should already be installed
# If not, install from project root:
pip install -r requirements.txt
```

## Pipeline Architecture

### Stage 1: Data Cleaning (`data_cleaning.py`)

The data cleaning stage performs 5 sequential processing steps:

#### Step 1: Job Zones Processing (Gatekeeper)
- **Input**: `Job Zones.xlsx`
- **Filter**: Job Zones 3 and 4 (targeting fresh graduates and interns)
- **Output**: Whitelist of valid O*NET-SOC codes
- **Purpose**: Creates the master list of jobs to process

#### Step 2: Occupation Data Processing (Identity)
- **Input**: `Occupation Data.xlsx`
- **Filter**: Only jobs in whitelist
- **Extract**: Title and Description columns
- **Output**: DataFrame with job identity information

#### Step 3: Interests Processing (Personality - RIASEC)
- **Input**: `Interests.xlsx`
- **Filter**: 
  - Jobs in whitelist
  - Scale ID = "OI" (Occupational Interest, excludes "IH" = Highpoint)
- **Extract**: Element Name (RIASEC category) and Data Value
- **Output**: DataFrame with RIASEC interest scores (1-7 scale)

#### Step 4: Knowledge Processing (Degree Matcher)
- **Input**: `Knowledge.xlsx`
- **Filter**:
  - Jobs in whitelist
  - Scale ID = "IM" (Importance)
  - Data Value > 3.0 (essential knowledge only)
- **Extract**: Element Name (knowledge domain) and Data Value
- **Output**: DataFrame with essential knowledge domains

#### Step 5: Technology Skills Processing (Resume Matcher)
- **Input**: `Technology Skills.xlsx`
- **Filter**: Jobs in whitelist
- **Extract**: Example column (specific tool names like "Python", "React")
- **Output**: DataFrame with technology/tool keywords

### Stage 2: Transformation (`transformation.py`)

The transformation stage converts cleaned DataFrames into structured JSON:

#### Station A: RIASEC Vector Transformation
- **Input**: Interests DataFrame (long format)
- **Process**: 
  - Pivot from long to wide format (6 rows per job → 1 row with 6 columns)
  - Normalize scores from 1-7 scale to 0.0-1.0 scale
- **Output**: DataFrame with O*NET-SOC Code and 6 RIASEC columns

#### Station B: Keyword Aggregation
- **Input**: Knowledge and Technology Skills DataFrames
- **Process**: Group by O*NET-SOC Code and aggregate into lists
- **Output**: 
  - Knowledge dictionary: `{job_id: [domain1, domain2, ...]}`
  - Tech skills dictionary: `{job_id: [skill1, skill2, ...]}`

#### Station C: Final Assembly
- **Input**: All processed data from previous stages
- **Process**: Combine all data into structured job profiles
- **Output**: List of job profile dictionaries

#### Packaging: JSON Export
- **Input**: List of job profiles
- **Output**: `output/jobs_database.json`

## Usage

### Running the Complete Pipeline

Run the transformation stage, which automatically calls the data cleaning stage:

```bash
cd ETL_pipeline
python transformation.py
```

Or from project root:

```bash
python -m ETL_pipeline.transformation
```

### Running Individual Stages

**Data Cleaning Only:**
```bash
cd ETL_pipeline
python data_cleaning.py
```

This returns a dictionary with all cleaned DataFrames (useful for testing or custom processing).

**Transformation Only:**
```bash
cd ETL_pipeline
python transformation.py
```

Note: Transformation automatically calls data cleaning if needed.

### Programmatic Usage

```python
from ETL_pipeline import data_cleaning, transformation

# Run data cleaning
data_package = data_cleaning.main()
# Returns: {
#     'whitelist': [...],
#     'occupation_df': DataFrame,
#     'interests_df': DataFrame,
#     'knowledge_df': DataFrame,
#     'technology_skills_df': DataFrame,
#     'job_zones_df': DataFrame
# }

# Run transformation
job_profiles = transformation.main()
# Returns: List of job profile dictionaries
```

## Output Format

The pipeline generates `output/jobs_database.json` with the following structure:

```json
[
  {
    "id": "11-1021.00",
    "title": "Data Scientist",
    "description": "Job description text...",
    "vectors": {
      "riasec": [0.17, 0.83, 0.33, 0.50, 0.83, 0.50]
    },
    "keywords": {
      "knowledge_domains": [
        "Computer Science",
        "Mathematics",
        "Statistics"
      ],
      "tech_skills": [
        "Python",
        "R",
        "SQL",
        "TensorFlow"
      ]
    },
    "filters": {
      "job_zone": 4
    }
  }
]
```

### Field Descriptions

- **id**: O*NET-SOC Code (unique identifier)
- **title**: Job title
- **description**: Job description text
- **vectors.riasec**: 6-dimensional personality vector (0.0-1.0 scale)
  - Order: Realistic, Investigative, Artistic, Social, Enterprising, Conventional
- **keywords.knowledge_domains**: List of essential knowledge domains (for degree matching)
- **keywords.tech_skills**: List of technology/tool keywords (for resume matching)
- **filters.job_zone**: Job zone (3 or 4 for fresh graduates/interns)

## Configuration

### Data Cleaning Configuration

Edit constants in `data_cleaning.py`:

```python
# Filter constants
VALID_JOB_ZONES = [3, 4]  # Target fresh graduates and interns
OCCUPATIONAL_INTEREST_SCALE = "OI"  # Occupational Interest scale
KNOWLEDGE_SCALE_ID = "IM"  # Importance scale
KNOWLEDGE_IMPORTANCE_THRESHOLD = 3.0  # Minimum importance for knowledge domains
```

### Transformation Configuration

Edit constants in `transformation.py`:

```python
# RIASEC normalization
RIASEC_MIN_SCALE = 1.0
RIASEC_MAX_SCALE = 7.0

# Output file
OUTPUT_DIR = Path(__file__).parent / "output"
OUTPUT_FILE = OUTPUT_DIR / "jobs_database.json"
```

## Testing

Run the test suite to validate the pipeline:

```bash
cd ETL_pipeline
python test_etl_pipeline.py
```

The test suite validates:
- Data cleaning module output structure
- Transformation module output structure
- Output JSON file validity
- Data integrity across stages
- RIASEC vector normalization
- Job profile completeness

## File Structure

```
ETL_pipeline/
├── data_cleaning.py          # Stage 1: Data cleaning and filtering
├── transformation.py          # Stage 2: Data transformation and JSON export
├── test_etl_pipeline.py       # Test suite
├── output/
│   └── jobs_database.json     # Generated output file
└── README.md                  # This file
```

## Dependencies

- **pandas**: Data manipulation and DataFrame operations
- **openpyxl**: Excel file reading (for O*NET database files)
- **pathlib**: Path handling (standard library)
- **json**: JSON serialization (standard library)

## Data Flow

```
O*NET Excel Files (from onet_job_scraping)
    ↓
[Stage 1: Data Cleaning]
    ├── Job Zones → Whitelist
    ├── Occupation Data → Titles & Descriptions
    ├── Interests → RIASEC scores
    ├── Knowledge → Knowledge domains
    └── Technology Skills → Tech skills
    ↓
[Stage 2: Transformation]
    ├── RIASEC Vector Transformation
    ├── Keyword Aggregation
    ├── Final Assembly
    └── JSON Export
    ↓
jobs_database.json (for Logic_Engine)
```

## Troubleshooting

### File Not Found Errors

**Error**: `FileNotFoundError: O*NET extraction base path not found`

**Solution**: Run the `onet_job_scraping` scraper first to download and extract O*NET database files:

```bash
python -m onet_job_scraping.main
```

### Empty DataFrames

**Error**: Empty DataFrames after filtering

**Possible Causes**:
- No jobs match the Job Zone filter (zones 3 or 4)
- O*NET database files are missing or corrupted
- Column names don't match expected format

**Solution**: Check the O*NET database files in `onet_job_scraping/output/extracted/`

### RIASEC Normalization Issues

**Error**: RIASEC values out of range (not 0.0-1.0)

**Solution**: Check that input RIASEC scores are in 1-7 scale. Normalization should convert to 0.0-1.0 automatically.

### Missing Knowledge Domains

**Note**: Knowledge domains are filtered by importance threshold (> 3.0). If a job has no knowledge domains, it means none exceeded the threshold. This is expected behavior.

## Performance

- **Data Cleaning**: ~5-10 seconds (depends on file sizes)
- **Transformation**: ~2-5 seconds
- **Total Pipeline**: ~7-15 seconds
- **Output File Size**: ~200-500 KB (depends on number of jobs)

## Integration

The ETL pipeline is designed to work with:

- **Input**: O*NET database Excel files from `onet_job_scraping`
- **Output**: JSON database consumed by `Logic_Engine` for job matching

The pipeline should be run whenever:
- O*NET database is updated
- Filter criteria change (e.g., different job zones)
- Knowledge threshold adjustments are made

## License

Part of the Final Year Project.

